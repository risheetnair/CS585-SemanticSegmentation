{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb272632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.2-cp38-cp38-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/risheetnair/opt/anaconda3/lib/python3.8/site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: numpy in /Users/risheetnair/opt/anaconda3/lib/python3.8/site-packages (from torchvision) (1.20.1)\n",
      "Requirement already satisfied: torch==2.2.2 in /Users/risheetnair/opt/anaconda3/lib/python3.8/site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/risheetnair/opt/anaconda3/lib/python3.8/site-packages (from torch==2.2.2->torchvision) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/risheetnair/opt/anaconda3/lib/python3.8/site-packages (from torch==2.2.2->torchvision) (1.8)\n",
      "Requirement already satisfied: networkx in /Users/risheetnair/opt/anaconda3/lib/python3.8/site-packages (from torch==2.2.2->torchvision) (2.5)\n",
      "Requirement already satisfied: jinja2 in /Users/risheetnair/opt/anaconda3/lib/python3.8/site-packages (from torch==2.2.2->torchvision) (2.11.3)\n",
      "Requirement already satisfied: fsspec in /Users/risheetnair/opt/anaconda3/lib/python3.8/site-packages (from torch==2.2.2->torchvision) (0.9.0)\n",
      "Requirement already satisfied: filelock in /Users/risheetnair/opt/anaconda3/lib/python3.8/site-packages (from torch==2.2.2->torchvision) (3.0.12)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/risheetnair/opt/anaconda3/lib/python3.8/site-packages (from jinja2->torch==2.2.2->torchvision) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/risheetnair/opt/anaconda3/lib/python3.8/site-packages (from networkx->torch==2.2.2->torchvision) (5.0.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/risheetnair/opt/anaconda3/lib/python3.8/site-packages (from sympy->torch==2.2.2->torchvision) (1.2.1)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f94587",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Implement the method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ffb5fcd5a3ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mclass_dict_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"class_dict.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mresolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m240\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mcamvid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCamVidDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CamVid/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_dict_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_dict_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresolution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# Example of loading a single sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ffb5fcd5a3ee>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, images_dir, labels_dir, class_dict_path, resolution, crop)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_class_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_dict_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlbl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ffb5fcd5a3ee>\u001b[0m in \u001b[0;36mparse_class_dict\u001b[0;34m(self, class_dict_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_class_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_dict_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# return a dictionary that maps class id (0-31) to a tuple ((R,G,B), class_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Implement the method\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrgb_to_class_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Implement the method"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "rev_normalize = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225], std=[1/0.229, 1/0.224, 1/0.225])\n",
    "\n",
    "def paired_crop_and_resize(image, label, size):\n",
    "    # Random crop\n",
    "    i, j, h, w = transforms.RandomResizedCrop.get_params(image, scale=(0.5, 1.0), ratio=(1, 1))\n",
    "    image = transforms.functional.resized_crop(image, i, j, h, w, size, Image.BILINEAR)\n",
    "    label = transforms.functional.resized_crop(label, i, j, h, w, size, Image.NEAREST)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "def paired_resize(image, label, size):\n",
    "    image = transforms.functional.resize(image, size, Image.BILINEAR)\n",
    "    label = transforms.functional.resize(label, size, Image.NEAREST)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, root, images_dir, labels_dir, class_dict_path, resolution, crop=False):\n",
    "        self.root = root\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.resolution = resolution\n",
    "        self.crop = crop\n",
    "        self.class_dict = self.parse_class_dict(os.path.join(root, class_dict_path))\n",
    "        self.images = [os.path.join(root, images_dir, img) for img in sorted(os.listdir(os.path.join(root, images_dir)))]\n",
    "        self.labels = [os.path.join(root, labels_dir, lbl) for lbl in sorted(os.listdir(os.path.join(root, labels_dir)))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label_path = self.labels[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = Image.open(label_path).convert(\"RGB\")\n",
    "        label = self.rgb_to_class_id(label)\n",
    "        if self.crop:\n",
    "            image, label = paired_crop_and_resize(image, label, self.resolution)\n",
    "        else:\n",
    "            image, label = paired_resize(image, label, self.resolution)\n",
    "            \n",
    "        # image to tensor and normalize\n",
    "        image = transforms.ToTensor()(image)\n",
    "        image = normalize(image)\n",
    "        label = torch.tensor(np.array(label)).long()\n",
    "        return image, label\n",
    "\n",
    "    def parse_class_dict(self, class_dict_path):\n",
    "        # return a dictionary that maps class id (0-31) to a tuple ((R,G,B), class_name)\n",
    "        class_dict = {}\n",
    "         with open(class_dict_path, mode='r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)\n",
    "            for i, line in enumerate(reader):\n",
    "                class_name, r, g, b = line\n",
    "                class_dict[i] = ((r, g, b), class_name)\n",
    "                \n",
    "        return class_dict\n",
    "                \n",
    "\n",
    "    def rgb_to_class_id(self, label_img):\n",
    "        # Convert an RGB label image to a class ID image (H, W, 3) -> (H, W)\n",
    "        raise NotImplementedError(\"Implement the method\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    images_dir = \"train/\"\n",
    "    labels_dir = \"train_labels/\"\n",
    "    class_dict_path = \"class_dict.csv\"\n",
    "    resolution = (240, 240)\n",
    "    camvid_dataset = CamVidDataset(root='CamVid/', images_dir=images_dir, labels_dir=labels_dir, class_dict_path=class_dict_path, resolution=resolution)\n",
    "\n",
    "    # Example of loading a single sample\n",
    "    image, label = camvid_dataset[0]\n",
    "\n",
    "    # To visualize or further process, you might want to convert 'label' back to a color image or directly use it for training a segmentation model.\n",
    "    label_vis = label.numpy().astype(np.float32)\n",
    "    label_vis /= 31.\n",
    "    label_vis *= 255.\n",
    "    label_vis = label_vis.astype(np.uint8)\n",
    "    label_vis = Image.fromarray(label_vis)\n",
    "    label_vis.save(\"label_vis.png\")\n",
    "    image_vis = transforms.functional.to_pil_image(rev_normalize(image))\n",
    "    image_vis.save(\"image_vis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c07300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
